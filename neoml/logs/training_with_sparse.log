======================================================================
NEOTIMMUML - TRAINING MODELS
======================================================================

[1/5] Loading data...
✓ Loaded 154,769 samples

[2/5] Encoding features (sparse)...
✓ Encoded to 262,228 features (sparse format)
✓ Memory efficiency: 0.02% non-zero
✓ Split: 123,815 train / 30,954 test

[3/5] Training models...
  [1/3] Random Forest - SKIPPED (memory constraints)
  [2/3] LightGBM (100 trees)... ✓
  [3/3] XGBoost (100 trees)... ✓

[4/5] Saving models...
  - RandomForest_model.joblib (skipped)
  ✓ LightGBM_model.joblib
  ✓ XGBoost_model.joblib

[5/5] Evaluating models on test set...
----------------------------------------------------------------------
Random Forest       : SKIPPED
LightGBM            : Acc=1.0000 | AUC=1.0000 | F1=1.0000
XGBoost             : Acc=1.0000 | AUC=1.0000 | F1=1.0000

[6/6] Creating output directories...
  ✓ output/SHAP_LightGBM/
  ✓ output/SHAP_XGBoost/

======================================================================
✓ ALL COMPLETE!
✓ Models saved in: /Users/vanetten/git_repos/neorefine/neoml
✓ Files created:
    - RandomForest_model.joblib
    - LightGBM_model.joblib
    - XGBoost_model.joblib
    - output/SHAP_LightGBM/ (directory)
    - output/SHAP_XGBoost/ (directory)
======================================================================
